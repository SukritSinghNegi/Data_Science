{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d939a52",
   "metadata": {},
   "source": [
    "# Vector Representation and Rule-Base matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f1fc0",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd2a440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl (777.4 MB)\n",
      "     ------------------------------------ 777.4/777.4 MB 613.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from en-core-web-lg==3.2.0) (3.2.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (21.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.7.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (60.9.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\krish\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f783f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "import collections\n",
    "from typing import Dict,List,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a73e1eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2bow(words:List[str],dictionary:Dict[str,str])->List[Tuple[int,int]]:\n",
    "    word_frequencies=collections.defaultdict(int)\n",
    "    for word in words:\n",
    "        if word not in dictionary:\n",
    "            dictionary[word]=len(dictionary)\n",
    "        word_frequencies[dictionary[word]]+=1\n",
    "    return list(word_frequencies.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522f2ab",
   "metadata": {},
   "source": [
    "Task a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86b9f96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1: This movie is very scary and long \n",
      " [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n",
      "\n",
      "\n",
      "Review 2: This movie is not scary and is slow \n",
      " [(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1)]\n",
      "\n",
      "\n",
      "Review 3: This movie is spooky and good \n",
      " [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text=['Review 1: This movie is very scary and long','Review 2: This movie is not scary and is slow','Review 3: This movie is spooky and good']\n",
    "for sample in sample_text:\n",
    "    dictionary={}\n",
    "    print(sample,'\\n',text2bow(sample.split(),dictionary))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ba3d7",
   "metadata": {},
   "source": [
    "statement1 : Review=(0,1),scary=(6,1)                                                    \n",
    "statement2 : Review=(0,1),scary=(6,1)                                                    \n",
    "statement3 : Review=(0,1)                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec333ce",
   "metadata": {},
   "source": [
    "Task 1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f828cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1: This movie is very scary and long \n",
      " {'Review': 0, '1:': 1, 'This': 2, 'movie': 3, 'is': 4, 'very': 5, 'scary': 6, 'and': 7, 'long': 8}\n",
      "\n",
      "\n",
      "Review 2: This movie is not scary and is slow \n",
      " {'Review': 0, '2:': 1, 'This': 2, 'movie': 3, 'is': 4, 'not': 5, 'scary': 6, 'and': 7, 'slow': 8}\n",
      "\n",
      "\n",
      "Review 3: This movie is spooky and good \n",
      " {'Review': 0, '3:': 1, 'This': 2, 'movie': 3, 'is': 4, 'spooky': 5, 'and': 6, 'good': 7}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text=['Review 1: This movie is very scary and long','Review 2: This movie is not scary and is slow','Review 3: This movie is spooky and good']\n",
    "for sample in sample_text:\n",
    "    dictionary={}\n",
    "    text2bow(sample.split(),dictionary)\n",
    "    print(sample,'\\n',dictionary)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa5522",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f912ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Siri\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher=Matcher(nlp.vocab)\n",
    "pattern=[{'LOWER':'hey'},{'LOWER':'siri'}]\n",
    "matcher.add('HeySiri',[pattern])\n",
    "doc=nlp(\"Hey, Siri! Hey Siri!\")\n",
    "matches= matcher(doc)\n",
    "for match_id,start,end in matches:\n",
    "    sting_id=nlp.vocab.strings[match_id]\n",
    "    span=doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62bfb131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, Siri\n"
     ]
    }
   ],
   "source": [
    "matcher=Matcher(nlp.vocab)\n",
    "pattern=[{'LOWER':'hey'},{'IS_PUNCT':True},{'LOWER':'siri'}]\n",
    "matcher.add('HeySiri',[pattern])\n",
    "doc=nlp(\"Hey, Siri! Hey Siri!\")\n",
    "matches= matcher(doc)\n",
    "for match_id,start,end in matches:\n",
    "    sting_id=nlp.vocab.strings[match_id]\n",
    "    span=doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409cf1db",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b435a403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text= apple ,Vector= True ,OOV= False\n",
      "Text= orange ,Vector= True ,OOV= False\n",
      "Text= pikkstn ,Vector= False ,OOV= True\n",
      "Text= german ,Vector= True ,OOV= False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "doc=nlp('apple orange pikkstn german')\n",
    "for token in doc:\n",
    "    print('Text=',token.text,',Vector=',token.has_vector,',OOV=',token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6829d2",
   "metadata": {},
   "source": [
    "They all are the part of the vocabolary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c257b",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9656e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotten mangoes\n",
      "sweet oranges\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in [\"ROTTEN mangoes\", \"sweet oranges\"]]\n",
    "matcher.add(\"Fruit\", patterns)\n",
    "doc = nlp(\"Do not put rotten mangoes and sweet oranges together.\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9441b34",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a04cf997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "I prefer 0.41702595353126526\n",
      "I the 0.3116089701652527\n",
      "I morning 0.3523589074611664\n",
      "I flight 0.19887085258960724\n",
      "I through 0.28335073590278625\n",
      "I Denmark 0.29523220658302307\n",
      "prefer I 0.41702595353126526\n",
      "prefer prefer 1.0\n",
      "prefer the 0.3199503719806671\n",
      "prefer morning 0.22616012394428253\n",
      "prefer flight 0.15512840449810028\n",
      "prefer through 0.23834623396396637\n",
      "prefer Denmark 0.06000184267759323\n",
      "the I 0.3116089701652527\n",
      "the prefer 0.3199503719806671\n",
      "the the 1.0\n",
      "the morning 0.3890417218208313\n",
      "the flight 0.25840193033218384\n",
      "the through 0.6090923547744751\n",
      "the Denmark -0.028872722759842873\n",
      "morning I 0.3523589074611664\n",
      "morning prefer 0.22616012394428253\n",
      "morning the 0.3890417218208313\n",
      "morning morning 1.0\n",
      "morning flight 0.36576882004737854\n",
      "morning through 0.3871423304080963\n",
      "morning Denmark 0.002022282686084509\n",
      "flight I 0.19887085258960724\n",
      "flight prefer 0.15512840449810028\n",
      "flight the 0.25840193033218384\n",
      "flight morning 0.36576882004737854\n",
      "flight flight 1.0\n",
      "flight through 0.2913546860218048\n",
      "flight Denmark 0.12891069054603577\n",
      "through I 0.28335073590278625\n",
      "through prefer 0.23834623396396637\n",
      "through the 0.6090923547744751\n",
      "through morning 0.3871423304080963\n",
      "through flight 0.2913546860218048\n",
      "through through 1.0\n",
      "through Denmark -0.007055560126900673\n",
      "Denmark I 0.29523220658302307\n",
      "Denmark prefer 0.06000184267759323\n",
      "Denmark the -0.028872722759842873\n",
      "Denmark morning 0.002022282686084509\n",
      "Denmark flight 0.12891069054603577\n",
      "Denmark through -0.007055560126900673\n",
      "Denmark Denmark 1.0\n"
     ]
    }
   ],
   "source": [
    "doc=nlp('I prefer the morning flight through Denmark')\n",
    "for token1 in doc:                                                      \n",
    "    for token2 in doc:                                                 \n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd616712",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06b71968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do Do 1.0\n",
      "Do not 0.7205888032913208\n",
      "Do put 0.6295328140258789\n",
      "Do rotten 0.23457235097885132\n",
      "Do mangoes 0.07004779577255249\n",
      "Do and 0.40728652477264404\n",
      "Do sweet 0.27546998858451843\n",
      "Do oranges 0.184742733836174\n",
      "Do together 0.450131356716156\n",
      "Do . 0.3056337237358093\n",
      "not Do 0.7205888032913208\n",
      "not not 1.0\n",
      "not put 0.6083958745002747\n",
      "not rotten 0.2806089520454407\n",
      "not mangoes 0.10161229223012924\n",
      "not and 0.5304263234138489\n",
      "not sweet 0.3388059437274933\n",
      "not oranges 0.17835381627082825\n",
      "not together 0.4486272633075714\n",
      "not . 0.4248487055301666\n",
      "put Do 0.6295328140258789\n",
      "put not 0.6083958745002747\n",
      "put put 1.0\n",
      "put rotten 0.31637996435165405\n",
      "put mangoes 0.12202073633670807\n",
      "put and 0.49793902039527893\n",
      "put sweet 0.3838925361633301\n",
      "put oranges 0.22029347717761993\n",
      "put together 0.6148674488067627\n",
      "put . 0.390465646982193\n",
      "rotten Do 0.23457235097885132\n",
      "rotten not 0.2806089520454407\n",
      "rotten put 0.31637996435165405\n",
      "rotten rotten 1.0\n",
      "rotten mangoes 0.3282413184642792\n",
      "rotten and 0.20274914801120758\n",
      "rotten sweet 0.35820987820625305\n",
      "rotten oranges 0.3876388669013977\n",
      "rotten together 0.203302800655365\n",
      "rotten . 0.13760977983474731\n",
      "mangoes Do 0.07004779577255249\n",
      "mangoes not 0.10161229223012924\n",
      "mangoes put 0.12202073633670807\n",
      "mangoes rotten 0.3282413184642792\n",
      "mangoes mangoes 1.0\n",
      "mangoes and 0.11509136110544205\n",
      "mangoes sweet 0.3816959261894226\n",
      "mangoes oranges 0.7255765795707703\n",
      "mangoes together 0.13692606985569\n",
      "mangoes . 0.03732023388147354\n",
      "and Do 0.40728652477264404\n",
      "and not 0.5304263234138489\n",
      "and put 0.49793902039527893\n",
      "and rotten 0.20274914801120758\n",
      "and mangoes 0.11509136110544205\n",
      "and and 1.0\n",
      "and sweet 0.41887080669403076\n",
      "and oranges 0.19245947897434235\n",
      "and together 0.5996914505958557\n",
      "and . 0.43241679668426514\n",
      "sweet Do 0.27546998858451843\n",
      "sweet not 0.3388059437274933\n",
      "sweet put 0.3838925361633301\n",
      "sweet rotten 0.35820987820625305\n",
      "sweet mangoes 0.3816959261894226\n",
      "sweet and 0.41887080669403076\n",
      "sweet sweet 1.0\n",
      "sweet oranges 0.4652591049671173\n",
      "sweet together 0.3930017352104187\n",
      "sweet . 0.2901315689086914\n",
      "oranges Do 0.184742733836174\n",
      "oranges not 0.17835381627082825\n",
      "oranges put 0.22029347717761993\n",
      "oranges rotten 0.3876388669013977\n",
      "oranges mangoes 0.7255765795707703\n",
      "oranges and 0.19245947897434235\n",
      "oranges sweet 0.4652591049671173\n",
      "oranges oranges 1.0\n",
      "oranges together 0.22797814011573792\n",
      "oranges . 0.14070969820022583\n",
      "together Do 0.450131356716156\n",
      "together not 0.4486272633075714\n",
      "together put 0.6148674488067627\n",
      "together rotten 0.203302800655365\n",
      "together mangoes 0.13692606985569\n",
      "together and 0.5996914505958557\n",
      "together sweet 0.3930017352104187\n",
      "together oranges 0.22797814011573792\n",
      "together together 1.0\n",
      "together . 0.3799852430820465\n",
      ". Do 0.3056337237358093\n",
      ". not 0.4248487055301666\n",
      ". put 0.390465646982193\n",
      ". rotten 0.13760977983474731\n",
      ". mangoes 0.03732023388147354\n",
      ". and 0.43241679668426514\n",
      ". sweet 0.2901315689086914\n",
      ". oranges 0.14070969820022583\n",
      ". together 0.3799852430820465\n",
      ". . 1.0\n"
     ]
    }
   ],
   "source": [
    "doc=nlp('Do not put rotten mangoes and sweet oranges together.')\n",
    "for token1 in doc:                                                      \n",
    "    for token2 in doc:                                                 \n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548e2c3",
   "metadata": {},
   "source": [
    "Task 6 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8690e96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text= Do ,Vector= True ,OOV= False\n",
      "Text= not ,Vector= True ,OOV= False\n",
      "Text= put ,Vector= True ,OOV= False\n",
      "Text= rotten ,Vector= True ,OOV= False\n",
      "Text= mangoes ,Vector= True ,OOV= False\n",
      "Text= and ,Vector= True ,OOV= False\n",
      "Text= sweet ,Vector= True ,OOV= False\n",
      "Text= oranges ,Vector= True ,OOV= False\n",
      "Text= together ,Vector= True ,OOV= False\n",
      "Text= . ,Vector= True ,OOV= False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print('Text=',token.text,',Vector=',token.has_vector,',OOV=',token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75549654",
   "metadata": {},
   "source": [
    "Task 6 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5da3fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mangoes oranges 0.7255765795707703\n"
     ]
    }
   ],
   "source": [
    "for token1 in doc:\n",
    "    if token1.text=='mangoes':\n",
    "        for token2 in doc:\n",
    "            if token2.text=='oranges':\n",
    "                print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1cab48",
   "metadata": {},
   "source": [
    "Task 6 c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aaf01d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet oranges 0.4652591049671173\n"
     ]
    }
   ],
   "source": [
    "for token1 in doc:\n",
    "    if token1.text=='sweet':\n",
    "        for token2 in doc:\n",
    "            if token2.text=='oranges':\n",
    "                print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e0443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
