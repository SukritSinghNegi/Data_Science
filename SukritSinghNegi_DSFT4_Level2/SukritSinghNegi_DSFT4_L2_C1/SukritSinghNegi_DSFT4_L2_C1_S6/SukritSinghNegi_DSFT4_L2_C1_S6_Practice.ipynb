{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36acda4",
   "metadata": {},
   "source": [
    "# Text Pre Processing Using spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "737855dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2106d",
   "metadata": {},
   "source": [
    "# Task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2445f1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['several',\n",
       " 'full',\n",
       " 'very',\n",
       " 'serious',\n",
       " 'name',\n",
       " 'nothing',\n",
       " 'rather',\n",
       " 'for',\n",
       " '‘ve',\n",
       " 'was',\n",
       " 'due',\n",
       " 'elsewhere',\n",
       " 'whenever',\n",
       " 'became',\n",
       " 'he']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = list(STOP_WORDS) \n",
    "stopwords[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7ce36da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb571574",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a897cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cries > cry > cri\n",
      "this > this > this\n",
      "lies > lie > lie\n",
      "computing > compute > comput\n",
      "organizing > organizing > organ\n",
      "matches > match > match\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer                                              \n",
    "stemmer = SnowballStemmer(language='english')   \n",
    "Tokens=['cries','this','lies','computing','organizing','matches']\n",
    "doc=nlp('cries this lies computing organizing matches')\n",
    "for token,d in zip(Tokens,doc):\n",
    "    print(token,'>',d.lemma_,'>',stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de833527",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b6d13",
   "metadata": {},
   "source": [
    "### Task 3 a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94952040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Note, poster, Kubrick, newsgroup, :, \n",
      "\n",
      ", found, bbs, ago, thought, pass, \n",
      ", Kubrick, freaks, ., \n",
      "\n",
      ", 02/23/89, \n",
      ", Transcriber, note, :, \n",
      "\n",
      ", Clarke, /, Kubrick/2001, fans, ,, \n",
      "\n",
      ", found, original, paper, copy, screenplay, felt, \n",
      ", compelled, transcribe, disk, upload, bulletin, \n",
      ", boards, enjoyment, ., \n",
      "\n",
      ", final, movie, deviates, screenplay, number, interesting, \n",
      ", ways, ., tried, maintain, format, original, document, \n",
      ", number, lines, page, original, ., order, reduce, \n",
      ", length, file, bar, \", ------, \", delimit, pages, \n",
      ", lot, whitespace, original, screenplay, page, .]\n"
     ]
    }
   ],
   "source": [
    "file=open('scifiscripts_intro.txt').read()\n",
    "doc1=nlp(file)\n",
    "new=[]\n",
    "for d in doc1:\n",
    "    if d.is_stop==False:\n",
    "        new.append(d)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ae6ba",
   "metadata": {},
   "source": [
    "### Task 3 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ecc2c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[small, paragraph, provided, input, text, pre, -, processing, quick, analysis, final, note, 10, key, sentences, academic, readers, expect, beginning, paragraph, placement, helps, readers, comprehend, position,  , try, ,, find, written, academic, piece, ,, textbook, scholarly, article, ,, reading, sentence, paragraph, able, follow, sequence, logic]\n"
     ]
    }
   ],
   "source": [
    "sentence=['A small paragraph is also provided as input for text pre-processing and quick analysis','A final note on 10 key sentences is that academic readers expect them to be at the beginning of the paragraph','This placement helps readers comprehend your position',' To try this, find a well written academic piece, such as a textbook or scholarly article, and go through part of it just by reading the first sentence of each paragraph','You should be able to follow the sequence of logic']\n",
    "new1=[]\n",
    "for s in sentence:\n",
    "    for t in nlp(s):\n",
    "        if t.is_stop==False:\n",
    "            new1.append(t)\n",
    "print(new1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f4debf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[small, paragraph, provided, input, text, pre, -, processing, quick, analysis], [final, note, 10, key, sentences, academic, readers, expect, beginning, paragraph], [placement, helps, readers, comprehend, position], [ , try, ,, find, written, academic, piece, ,, textbook, scholarly, article, ,, reading, sentence, paragraph], [able, follow, sequence, logic]]\n"
     ]
    }
   ],
   "source": [
    "sentence=['A small paragraph is also provided as input for text pre-processing and quick analysis','A final note on 10 key sentences is that academic readers expect them to be at the beginning of the paragraph','This placement helps readers comprehend your position',' To try this, find a well written academic piece, such as a textbook or scholarly article, and go through part of it just by reading the first sentence of each paragraph','You should be able to follow the sequence of logic']\n",
    "new1=[]\n",
    "for s in sentence:\n",
    "    new=[]\n",
    "    for t in nlp(s):\n",
    "        if t.is_stop==False:\n",
    "            new.append(t)\n",
    "    new1.append(new)\n",
    "print(new1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dac838",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "790ccfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryan > PROPN > NNP\n",
      "visited > VERB > VBD\n",
      "his > PRON > PRP$\n",
      "friend > NOUN > NN\n",
      "for > ADP > IN\n",
      "a > DET > DT\n",
      "while > NOUN > NN\n",
      "and > CCONJ > CC\n",
      "then > ADV > RB\n",
      "went > VERB > VBD\n",
      "home > ADV > RB\n",
      "at > ADP > IN\n",
      "10 > NUM > CD\n",
      "pm > NOUN > NN\n",
      ". > PUNCT > .\n"
     ]
    }
   ],
   "source": [
    "text=nlp('Bryan visited his friend for a while and then went home at 10pm.')\n",
    "text1=[]\n",
    "for t in text:\n",
    "    print(t,'>',t.pos_,'>',t.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3841a8a",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "84187fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propernoun [PADUA, HIGH, SCHOOL, DAY, Revision, November]\n",
      "number [12, 1997]\n"
     ]
    }
   ],
   "source": [
    "file1=open('Random.txt').read()\n",
    "doc2=nlp(file1)\n",
    "propernoun=[]\n",
    "number=[]\n",
    "for d in doc2:\n",
    "    if d.pos_=='PROPN':\n",
    "        propernoun.append(d)\n",
    "    elif d.pos_=='NUM':\n",
    "        number.append(d)\n",
    "    else:\n",
    "        continue\n",
    "print('propernoun',propernoun)\n",
    "print('number',number)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536965f5",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a6f9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words |={'stop1','stop2','stop3','stop4','stop5'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89db4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'several', 'full', 'very', 'serious', 'name', 'nothing', 'rather', 'for', '‘ve', 'was', 'due', 'elsewhere', 'whenever', 'became', 'between', 'he', 'make', 'nobody', 'else', 'since', 'ours', 'too', 'stop2', 'thereafter', 'used', 'both', 'seemed', 'any', 'hereupon', 'has', 'becoming', 'whereafter', 'among', 'were', 'beforehand', 'twenty', 'what', 'herein', 'off', 'being', 'otherwise', 'to', 'that', 'noone', 'other', 'afterwards', 'upon', 'everywhere', 'give', 'and', 'former', 'all', 'forty', 'mostly', 'thru', 'toward', 'everyone', 'up', 'empty', 'yet', 'please', 'somehow', 'than', 'why', 'anyhow', 'next', 'of', 'six', 'only', 'made', 'mine', 'into', 'five', 'wherever', 'along', 'but', '’s', \"'m\", 'say', 'sometimes', 'with', 'no', 'from', 'sometime', 'then', 'how', 'ourselves', 'behind', 'the', 'top', 'hereby', \"n't\", 'can', 'move', 'now', '‘m', 'already', 'latterly', 'where', 'an', 'however', 'nowhere', 'various', 'two', 'formerly', 'go', 'ten', 'this', 'around', 'most', 'side', 'own', 'via', '’ll', 'some', 'your', 'did', 'whole', 'hereafter', 'therein', 'seems', 'anything', '‘re', 'whoever', 'whatever', 'within', 'eight', 'fifteen', 'yourself', 'much', 'himself', 'be', 'part', 'alone', 'either', 'will', 'there', 'on', 'regarding', 'thereby', 'is', 'per', 'their', 'been', 'many', 'herself', 'four', 'by', 'well', 'so', 'each', 'whence', 'thus', 'amount', 'every', 'myself', 'though', 'same', 'enough', 'whom', 'show', 'nevertheless', 'onto', 'when', 'does', 'themselves', '’ve', 'call', 'never', 'whereas', 'seem', 'these', 'could', 'using', 'a', '’re', 'always', 'them', 'not', 'almost', 'first', 'are', 'just', 'thereupon', \"'re\", 'out', 'after', 'put', 'still', 'besides', 'none', 'across', 'its', 'whereupon', 'his', 'indeed', 'becomes', 'meanwhile', '’d', 'eleven', 'those', 'here', 'against', 're', 'therefore', 'hers', 'n’t', 'you', 'am', 'may', 'during', 'bottom', 'beside', 'in', 'further', 'i', 'had', 'without', 'stop1', 'over', 'we', 'must', 'keep', 'ca', 'me', 'throughout', 'would', 'n‘t', 'really', 'done', 'again', 'our', 'everything', 'become', 'twelve', 'last', 'her', 'nine', 'once', 'anyone', 'if', '‘d', 'together', 'she', 'stop4', 'him', 'through', 'who', 'back', 'down', 'nor', 'under', 'stop3', 'take', 'although', 'my', 'namely', 'beyond', '‘ll', 'at', 'whose', 'anywhere', 'might', 'which', '‘s', 'moreover', 'have', 'fifty', 'above', 'should', 'one', 'they', 'amongst', 'before', '’m', 'or', 'neither', 'doing', 'itself', 'because', 'unless', \"'s\", 'anyway', 'somewhere', 'get', \"'ve\", 'another', 'also', 'hence', 'front', 'less', \"'d\", 'thence', 'others', 'about', 'whither', 'see', 'seeming', 'as', 'until', 'yours', 'more', 'third', 'it', 'hundred', 'except', 'sixty', 'us', 'even', 'something', \"'ll\", 'do', 'whereby', 'few', 'ever', 'whether', 'such', 'towards', 'below', 'often', 'quite', 'latter', 'yourselves', 'cannot', 'someone', 'three', 'least', 'perhaps', 'while', 'stop5', 'wherein'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da556fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words -={'always','never','between','becomes'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "580fb1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'several', 'full', 'very', 'serious', 'name', 'nothing', 'rather', 'for', '‘ve', 'was', 'due', 'elsewhere', 'whenever', 'became', 'he', 'make', 'nobody', 'else', 'since', 'ours', 'too', 'stop2', 'thereafter', 'used', 'both', 'seemed', 'any', 'hereupon', 'has', 'becoming', 'whereafter', 'among', 'were', 'beforehand', 'twenty', 'what', 'herein', 'off', 'being', 'otherwise', 'to', 'that', 'noone', 'other', 'afterwards', 'upon', 'everywhere', 'give', 'and', 'former', 'all', 'forty', 'mostly', 'thru', 'toward', 'everyone', 'up', 'empty', 'yet', 'please', 'somehow', 'than', 'why', 'anyhow', 'next', 'of', 'six', 'only', 'made', 'mine', 'into', 'five', 'wherever', 'along', 'but', '’s', \"'m\", 'say', 'sometimes', 'with', 'no', 'from', 'sometime', 'then', 'how', 'ourselves', 'behind', 'the', 'top', 'hereby', \"n't\", 'can', 'move', 'now', '‘m', 'already', 'latterly', 'where', 'an', 'however', 'nowhere', 'various', 'two', 'formerly', 'go', 'ten', 'this', 'around', 'most', 'side', 'own', 'via', '’ll', 'some', 'your', 'did', 'whole', 'hereafter', 'therein', 'seems', 'anything', '‘re', 'whoever', 'whatever', 'within', 'eight', 'fifteen', 'yourself', 'much', 'himself', 'be', 'part', 'alone', 'either', 'will', 'there', 'on', 'regarding', 'thereby', 'is', 'per', 'their', 'been', 'many', 'herself', 'four', 'by', 'well', 'so', 'each', 'whence', 'thus', 'amount', 'every', 'myself', 'though', 'same', 'enough', 'whom', 'show', 'nevertheless', 'onto', 'when', 'does', 'themselves', '’ve', 'call', 'whereas', 'seem', 'these', 'could', 'using', 'a', '’re', 'them', 'not', 'almost', 'first', 'are', 'just', 'thereupon', \"'re\", 'out', 'after', 'put', 'still', 'besides', 'none', 'across', 'its', 'whereupon', 'his', 'indeed', 'meanwhile', '’d', 'eleven', 'those', 'here', 'against', 're', 'therefore', 'hers', 'n’t', 'you', 'am', 'may', 'during', 'bottom', 'beside', 'in', 'further', 'i', 'had', 'without', 'stop1', 'over', 'we', 'must', 'keep', 'ca', 'me', 'throughout', 'would', 'n‘t', 'really', 'done', 'again', 'our', 'everything', 'become', 'twelve', 'last', 'her', 'nine', 'once', 'anyone', 'if', '‘d', 'together', 'she', 'stop4', 'him', 'through', 'who', 'back', 'down', 'nor', 'under', 'stop3', 'take', 'although', 'my', 'namely', 'beyond', '‘ll', 'at', 'whose', 'anywhere', 'might', 'which', '‘s', 'moreover', 'have', 'fifty', 'above', 'should', 'one', 'they', 'amongst', 'before', '’m', 'or', 'neither', 'doing', 'itself', 'because', 'unless', \"'s\", 'anyway', 'somewhere', 'get', \"'ve\", 'another', 'also', 'hence', 'front', 'less', \"'d\", 'thence', 'others', 'about', 'whither', 'see', 'seeming', 'as', 'until', 'yours', 'more', 'third', 'it', 'hundred', 'except', 'sixty', 'us', 'even', 'something', \"'ll\", 'do', 'whereby', 'few', 'ever', 'whether', 'such', 'towards', 'below', 'often', 'quite', 'latter', 'yourselves', 'cannot', 'someone', 'three', 'least', 'perhaps', 'while', 'stop5', 'wherein'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f31f2c",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba3650c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PADUA, HIGH, SCHOOL, -, DAY, \n",
      ", Revision, November, 12, ,, 1997, \n",
      ", hope, dinner, ready, minutes, Mrs., Johnson, squirts, screamer, ., \n",
      ", grabs, mail, rifles, ,, bends, kiss, Sharon, cheek, ., \n",
      ", MICHAEL-, C'm, ., supposed, tour, ., head, office, \n",
      ", MICHAEL, (, continuing)-, --, Dakota, ?, \n",
      "          \n",
      "                                 \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "file2=open('Raw_data_for_analysis.txt').read()\n",
    "# Tokenazation\n",
    "doc3=nlp(file2)\n",
    "#remove stopwords\n",
    "new2=[]\n",
    "for d in doc3:\n",
    "    if d.is_stop==False:\n",
    "        new2.append(d)\n",
    "print(new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d238a902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA > PROPN > NNP\n",
      "HIGH > PROPN > NNP\n",
      "SCHOOL > PROPN > NNP\n",
      "- > PUNCT > HYPH\n",
      "DAY > PROPN > NNP\n",
      "\n",
      " > SPACE > _SP\n",
      "Revision > PROPN > NNP\n",
      "November > PROPN > NNP\n",
      "12 > NUM > CD\n",
      ", > PUNCT > ,\n",
      "1997 > NUM > CD\n",
      "\n",
      " > SPACE > _SP\n",
      "I > PRON > PRP\n",
      "hope > VERB > VBP\n",
      "dinner > NOUN > NN\n",
      "'s > PART > POS\n",
      "ready > ADJ > JJ\n",
      "because > SCONJ > IN\n",
      "I > PRON > PRP\n",
      "only > ADV > RB\n",
      "have > VERB > VBP\n",
      "ten > NUM > CD\n",
      "minutes > NOUN > NNS\n",
      "before > SCONJ > IN\n",
      "Mrs. > PROPN > NNP\n",
      "Johnson > PROPN > NNP\n",
      "squirts > VERB > VBZ\n",
      "out > ADP > RP\n",
      "a > DET > DT\n",
      "screamer > NOUN > NN\n",
      ". > PUNCT > .\n",
      "\n",
      " > SPACE > _SP\n",
      "He > PRON > PRP\n",
      "grabs > VERB > VBZ\n",
      "the > DET > DT\n",
      "mail > NOUN > NN\n",
      "and > CCONJ > CC\n",
      "rifles > NOUN > NNS\n",
      "through > ADP > IN\n",
      "it > PRON > PRP\n",
      ", > PUNCT > ,\n",
      "as > SCONJ > IN\n",
      "he > PRON > PRP\n",
      "bends > VERB > VBZ\n",
      "down > ADP > RP\n",
      "to > PART > TO\n",
      "kiss > VERB > VB\n",
      "Sharon > PROPN > NNP\n",
      "on > ADP > IN\n",
      "the > DET > DT\n",
      "cheek > NOUN > NN\n",
      ". > PUNCT > .\n",
      "\n",
      " > SPACE > _SP\n",
      "MICHAEL- > PROPN > NNP\n",
      "C'm > VERB > VBZ\n",
      "on > ADP > RP\n",
      ". > PUNCT > .\n",
      "I > PRON > PRP\n",
      "'m > AUX > VBP\n",
      "supposed > VERB > VBN\n",
      "to > PART > TO\n",
      "give > VERB > VB\n",
      "you > PRON > PRP\n",
      "the > DET > DT\n",
      "tour > NOUN > NN\n",
      ". > PUNCT > .\n",
      "They > PRON > PRP\n",
      "head > VERB > VBP\n",
      "out > ADP > IN\n",
      "of > ADP > IN\n",
      "the > DET > DT\n",
      "office > NOUN > NN\n",
      "\n",
      " > SPACE > _SP\n",
      "MICHAEL > PROPN > NNP\n",
      "( > PUNCT > -LRB-\n",
      "continuing)- > NOUN > NNS\n",
      "So > ADV > RB\n",
      "-- > PUNCT > :\n",
      "which > PRON > WDT\n",
      "Dakota > PROPN > NNP\n",
      "you > PRON > PRP\n",
      "from > ADP > IN\n",
      "? > PUNCT > .\n",
      "\n",
      "          \n",
      "                                 \n",
      " > SPACE > _SP\n"
     ]
    }
   ],
   "source": [
    "# pos_tag_\n",
    "for d in doc3:\n",
    "    print(d,'>',d.pos_,'>',d.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da56a0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA > PADUA\n",
      "HIGH > HIGH\n",
      "SCHOOL > SCHOOL\n",
      "- > -\n",
      "DAY > DAY\n",
      "\n",
      " > \n",
      "\n",
      "Revision > Revision\n",
      "November > November\n",
      "12 > 12\n",
      ", > ,\n",
      "1997 > 1997\n",
      "\n",
      " > \n",
      "\n",
      "I > I\n",
      "hope > hope\n",
      "dinner > dinner\n",
      "'s > 's\n",
      "ready > ready\n",
      "because > because\n",
      "I > I\n",
      "only > only\n",
      "have > have\n",
      "ten > ten\n",
      "minutes > minute\n",
      "before > before\n",
      "Mrs. > Mrs.\n",
      "Johnson > Johnson\n",
      "squirts > squirt\n",
      "out > out\n",
      "a > a\n",
      "screamer > screamer\n",
      ". > .\n",
      "\n",
      " > \n",
      "\n",
      "He > he\n",
      "grabs > grab\n",
      "the > the\n",
      "mail > mail\n",
      "and > and\n",
      "rifles > rifle\n",
      "through > through\n",
      "it > it\n",
      ", > ,\n",
      "as > as\n",
      "he > he\n",
      "bends > bend\n",
      "down > down\n",
      "to > to\n",
      "kiss > kiss\n",
      "Sharon > Sharon\n",
      "on > on\n",
      "the > the\n",
      "cheek > cheek\n",
      ". > .\n",
      "\n",
      " > \n",
      "\n",
      "MICHAEL- > MICHAEL-\n",
      "C'm > come\n",
      "on > on\n",
      ". > .\n",
      "I > I\n",
      "'m > be\n",
      "supposed > suppose\n",
      "to > to\n",
      "give > give\n",
      "you > you\n",
      "the > the\n",
      "tour > tour\n",
      ". > .\n",
      "They > they\n",
      "head > head\n",
      "out > out\n",
      "of > of\n",
      "the > the\n",
      "office > office\n",
      "\n",
      " > \n",
      "\n",
      "MICHAEL > MICHAEL\n",
      "( > (\n",
      "continuing)- > continuing)-\n",
      "So > so\n",
      "-- > --\n",
      "which > which\n",
      "Dakota > Dakota\n",
      "you > you\n",
      "from > from\n",
      "? > ?\n",
      "\n",
      "          \n",
      "                                 \n",
      " > \n",
      "          \n",
      "                                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "for d in doc3:\n",
    "    print(d.text,'>',d.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff03b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
